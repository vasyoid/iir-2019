{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_url, doc_id, sz_bytes, sz_words):\n",
    "        self.url = doc_url       # document url\n",
    "        self.id = doc_id         # unique document id (str)\n",
    "        self.sz_bytes = sz_bytes # document size in bytes before deleting html markup\n",
    "        self.sz_words = sz_words # number of words in document before deleting html markup\n",
    "        self.words = []          # list of words in document after deleting html markup\n",
    "        self.links = []          # lisk of links in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "DOCUMENT_TAG = \"<document>\"\n",
    "DOC_URL_TAG = \"<docURL>\"\n",
    "DOC_ID_TAG = \"<docID>\"\n",
    "\n",
    "def preprocess_doc(doc, doc_url, doc_id):\n",
    "    document = Document(doc_url, doc_id, len(doc), len(doc.split()))\n",
    "    try:\n",
    "        soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    except:\n",
    "        print(\"incorrect document:\", doc_url, doc_id)\n",
    "        return document\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        document.links.append(link[\"href\"])\n",
    "    document.words = soup.get_text(separator=\" \").split()\n",
    "    return document\n",
    "    \n",
    "def preprocess_file(d, f):\n",
    "    print(\"preprocessing\", os.path.join(d, f))\n",
    "    with open(os.path.join(d, f), \"rt\", encoding=\"cp1251\") as fin, open(os.path.join(d, f.replace(\".xml\", \".out\")), \"wb\") as fout:\n",
    "        for line in fin:\n",
    "            if line.startswith(DOCUMENT_TAG):\n",
    "                doc = line[37:-11]\n",
    "            elif line.startswith(DOC_URL_TAG):\n",
    "                doc_url = line[8:-10]\n",
    "            elif line.startswith(DOC_ID_TAG):\n",
    "                doc_id = line[7:-20]\n",
    "                document = preprocess_doc(base64.b64decode(doc), base64.b64decode(doc_url), doc_id)\n",
    "                pickle.dump(document, fout)\n",
    "\n",
    "def preprocess_collection(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if (file.endswith(\".xml\")):\n",
    "            preprocess_file(directory, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class BaseDocumentProcessor:\n",
    "    def process(self, document):\n",
    "        pass\n",
    "    def result(self):\n",
    "        pass\n",
    "\n",
    "def process_file(d, f, processor, pbar):\n",
    "    print(\"processing\", os.path.join(d, f))\n",
    "    with open(os.path.join(d, f), \"rb\") as fin:\n",
    "        while True:\n",
    "             pbar.update(1)\n",
    "            try:\n",
    "                document = pickle.load(fin)\n",
    "            except:\n",
    "                break\n",
    "            processor.process(document)\n",
    "\n",
    "def process_collection(directory, processor):\n",
    "    pbar = tqdm(total = 200000)\n",
    "    for file in os.listdir(directory):\n",
    "        if (file.endswith(\".out\")):\n",
    "            process_file(directory, file, processor, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "\n",
    "COLLECTION_DIRECTORY = \"byweb\" # directory with .out files to process\n",
    "\n",
    "class PrimaryStatsGetter(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "        self._docs_count = 0\n",
    "        self._sz_bytes_html = []\n",
    "        self._sz_words_html = []\n",
    "        self._sz_bytes = []\n",
    "        self._sz_words = []\n",
    "    \n",
    "    def process(self, document):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        self._docs_count += 1\n",
    "        self._sz_bytes_html.append(document.sz_bytes)\n",
    "        self._sz_words_html.append(document.sz_words)\n",
    "        self._sz_bytes.append(len(\"\".join(document.words)))\n",
    "        self._sz_words.append(len(document.words))\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\" summarize and output all data \"\"\"\n",
    "        size_ratio = [x / y for x, y in zip(self._sz_bytes, self._sz_bytes_html)]\n",
    "        print(\"total documents:\", self._docs_count)\n",
    "        print(\"average doc size in bytes:\", round(mean(self._sz_bytes_html), 0))\n",
    "        print(\"average text size in bytes:\", round(mean(self._sz_bytes), 0))\n",
    "        print(\"average word count including html:\", round(mean(self._sz_words_html), 0))\n",
    "        print(\"average word count:\", round(mean(self._sz_words), 0))\n",
    "        print(\"average size ratio:\", mean(size_ratio))\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(self._sz_bytes_html, bins=100, range=(0, 200000))\n",
    "        plt.legend(labels=(\"Размер документа с разметкой (в байтах)\",), fontsize=\"x-large\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(self._sz_bytes, bins=100, range=(0, 20000))\n",
    "        plt.legend(labels=(\"Размер документа без разметки (в байтах)\",), fontsize=\"x-large\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(self._sz_words_html, bins=100, range=(0, 10000))\n",
    "        plt.legend(labels=(\"Количество слов в документе с разметкой\",), fontsize=\"x-large\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(self._sz_words, bins=100, range=(0, 4000))\n",
    "        plt.legend(labels=(\"Количество слов в документе без разметки\",), fontsize=\"x-large\")\n",
    "        \n",
    "        plt.figure(figsize=(16, 5))\n",
    "        plt.hist(size_ratio, bins=100)\n",
    "        plt.legend(labels=(\"Отношение объема текста к объему исходного документа\",), fontsize=\"x-large\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "processor = PrimaryStatsGetter()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_DIRECTORY = \"byweb\" # directory with .out files to process\n",
    "document_urls = {}\n",
    "\n",
    "class GetDocUrls(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "    \n",
    "    def process(self, document):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        document_url = str(document.url)[2:-1].split(\"?\")[0].split(\"#\")[0]\n",
    "        if document_url[-1] == '/':\n",
    "            document_url = document_url[:-1]\n",
    "        document_urls[document_url] = True\n",
    "        \n",
    "    def result(self):\n",
    "        pass\n",
    "        \n",
    "processor = GetDocUrls()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "COLLECTION_DIRECTORY = \"byweb\"\n",
    "\n",
    "stop_words = [\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между',\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you\\'re', 'you\\'ve', 'you\\'ll', 'you\\'d', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she\\'s', 'her', 'hers', 'herself', 'it', 'it\\'s', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that\\'ll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'don\\'t', 'should', 'should\\'ve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'aren\\'t', 'couldn', 'couldn\\'t', 'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', 'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t', 'isn', 'isn\\'t', 'ma', 'mightn', 'mightn\\'t', 'mustn', 'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t', 'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren', 'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "]\n",
    "\n",
    "class MystemProcessor(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        self.m = Mystem(grammar_info=False, disambiguation=False)\n",
    "        reg = re.compile('[a-zа-яё0-9]')\n",
    "        self.latin_reg = re.compile('[a-z]')\n",
    "        self.filterFunc = lambda w : reg.match(w)\n",
    "        self.total_words_count = 0\n",
    "        self.stop_words_count = 0\n",
    "        self.total_symbol_count = 0\n",
    "        self.latin_words_count = 0\n",
    "        self.words_to_count = dict()\n",
    "        self.words_to_document = dict()\n",
    "        self.doc_count = 0\n",
    "    \n",
    "    def process(self, document):\n",
    "        self.doc_count += 1\n",
    "        if (self.doc_count % 10000 == 0):\n",
    "            print(self.doc_count)\n",
    "        text = ' '.join(document.words).lower()\n",
    "        lemmas = self.m.lemmatize(text)\n",
    "        words = list(filter(self.filterFunc, lemmas))\n",
    "    \n",
    "\n",
    "        for word in words:\n",
    "            self.total_words_count += 1\n",
    "            self.stop_words_count += 1 if word in stop_words else 0\n",
    "            self.total_symbol_count += len(word)\n",
    "            self.latin_words_count += 1 if self.latin_reg.match(word) else 0\n",
    "            if (word in self.words_to_count):\n",
    "                self.words_to_count[word] += 1\n",
    "            else:\n",
    "                self.words_to_count[word] = 1\n",
    "                \n",
    "            if (word in self.words_to_document):\n",
    "                self.words_to_document[word].add(document.id)\n",
    "            else:\n",
    "                self.words_to_document[word] = { document.id }\n",
    "        \n",
    "         \n",
    "    def result(self):\n",
    "        words = list(self.words_to_document)\n",
    "        print('Доля стоп слов в коллекции', self.stop_words_count / self.total_words_count)\n",
    "        print('Доля слов латиницей в коллекции', self.latin_words_count / self.total_words_count)\n",
    "        print('Средняя длина слова в коллекции: ', self.total_symbol_count / self.total_words_count)\n",
    "\n",
    "        idf = [ (word, math.log(200000 / len(self.words_to_document[word]), 2)) for word in words ]\n",
    "        cf = [(word, self.words_to_count[word] / self.total_words_count) for word in words ]\n",
    "\n",
    "        symb = mean([len(w) for w in words])\n",
    "        print('Средняя длина слова в словаре: ', symb)\n",
    "\n",
    "        with open (\"words.tmp\", \"wb\") as fout:\n",
    "            pickle.dump(words, fout)\n",
    "            pickle.dump(cf, fout)\n",
    "            pickle.dump(idf, fout)\n",
    "\n",
    "mystem_processor = MystemProcessor()\n",
    "process_collection(COLLECTION_DIRECTORY, mystem_processor)\n",
    "mystem_processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open (\"words.tmp\", \"rb\") as fin:\n",
    "    words = pickle.load(fin)\n",
    "    cf = pickle.load(fin)\n",
    "    idf = pickle.load(fin)\n",
    "    \n",
    "sortkey = lambda x : x[1]\n",
    "\n",
    "idf.sort(key = sortkey, reverse = False)\n",
    "print('idf по возрастанию')\n",
    "print(*idf[:20], sep=\"\\n\")\n",
    "\n",
    "idf.sort(key = sortkey, reverse = True)\n",
    "print('idf по убыванию')\n",
    "print(*idf[:20], sep=\"\\n\")\n",
    "\n",
    "cf.sort(key = sortkey, reverse = False)\n",
    "print('cf по возрастанию')\n",
    "print(*cf[:20], sep=\"\\n\")\n",
    "\n",
    "cf.sort(key = sortkey, reverse = True)\n",
    "print('cf по убыванию')\n",
    "print(*cf[:20], sep=\"\\n\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "cf_rank = fig.add_subplot(1, 1, 1)\n",
    "cf_rank.set_xlabel('log(ранг слова)')\n",
    "cf_rank.set_ylabel('log(частота слова)')\n",
    "cf_rank.set_xscale('log')\n",
    "cf_rank.set_yscale('log')\n",
    "cf_rank.plot([y for x, y in cf], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_DIRECTORY = \"byweb\" # directory with .out files to process\n",
    "document_urls = {}\n",
    "\n",
    "class GetDocUrls(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "    \n",
    "    def process(self, document):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        document_url = str(document.url)[2:-1].split(\"?\")[0].split(\"#\")[0]\n",
    "        if document_url[-1] == '/':\n",
    "            document_url = document_url[:-1]\n",
    "        document_urls[document_url] = True\n",
    "        \n",
    "    def result(self):\n",
    "        pass\n",
    "        \n",
    "processor = GetDocUrls()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "COLLECTION_DIRECTORY = \"byweb\" # directory with .out files to process\n",
    "\n",
    "class GraphBuider(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "        self.edge_list = []\n",
    "    \n",
    "    def process(self, document):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        links = document.links\n",
    "        document_url = str(document.url)[2:-1].split(\"?\")[0].split(\"#\")[0]\n",
    "        if document_url[-1] == '/':\n",
    "            document_url = document_url[:-1]\n",
    "        links_absolute_no_params = []\n",
    "        for link in links:\n",
    "            if not link:\n",
    "                continue\n",
    "            link_absolute_path = link    \n",
    "            if not (link_absolute_path.startswith(\"http://\") or link_absolute_path.startswith(\"https://\")):\n",
    "                if link_absolute_path[0] == '.':\n",
    "                    link_absolute_path = link_absolute_path[2:]\n",
    "                link_absolute_path = document_url.rsplit(\"/\", 1)[0] + \"/\" + link_absolute_path\n",
    "            link_absolute_no_params = link_absolute_path.split(\"?\")[0].split(\"#\")[0]\n",
    "            if link_absolute_no_params[-1] == '/':\n",
    "                    link_absolute_no_params = link_absolute_no_params[:-1]\n",
    "            if not (link_absolute_no_params in links_absolute_no_params):\n",
    "                links_absolute_no_params.append(link_absolute_no_params)\n",
    "        for link in links_absolute_no_params:\n",
    "            if link in document_urls:\n",
    "                self.edge_list.append((document_url, link))\n",
    "        \n",
    "    def result(self):\n",
    "        graph = nx.DiGraph(self.edge_list)\n",
    "        top_300_page_rank = sorted(nx.pagerank(graph).items(), key=operator.itemgetter(1), reverse=True)[:300]\n",
    "        top_300_page_rank = [x[0] for x in top_300_page_rank]\n",
    "        with open(\"graph4.csv\", \"wt\") as fout:\n",
    "            for in_vertex, out_vertex in tqdm_notebook(graph.edges()):\n",
    "                if in_vertex in top_300_page_rank and out_vertex in top_300_page_rank:\n",
    "                    fout.write(in_vertex)\n",
    "                    fout.write(\";\")\n",
    "                    fout.write(out_vertex)\n",
    "                    fout.write('\\n')    \n",
    "        \n",
    "processor = GraphBuider()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "processor.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
