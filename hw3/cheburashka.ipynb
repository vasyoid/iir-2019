{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This cell contains all constants thay may differ on our machines '''\n",
    "\n",
    "ELASTIC_HOST = 'localhost'\n",
    "ELASTIC_PORT = 9200\n",
    "COLLECTION_DIRECTORY = \"../byweb\" # directory with .out files to process\n",
    "COLLECTION_DIRECTORY_MYSTEM = \"../byweb_stem\" # directory with .out files after mystem processing\n",
    "\n",
    "QUERIES_FILE = \"../web2008_adhoc.xml\"\n",
    "RELEVANCE_FILE = \"../relevant_table_2009.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_url, doc_id, sz_bytes, sz_words):\n",
    "        self.url = doc_url       # document url\n",
    "        self.id = doc_id         # unique document id (str)\n",
    "        self.sz_bytes = sz_bytes # document size in bytes before deleting html markup\n",
    "        self.sz_words = sz_words # number of words in document before deleting html markup\n",
    "        self.words = []          # list of words in document after deleting html markup\n",
    "        self.links = []          # lisk of links in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "class BaseDocumentProcessor:\n",
    "    def process(self, document, title):\n",
    "        pass\n",
    "    def result(self):\n",
    "        pass\n",
    "    def process_pack(self):\n",
    "        pass\n",
    "\n",
    "def process_file(d, f, processor, pbar):\n",
    "    print(\"processing\", os.path.join(d, f))\n",
    "    with open(os.path.join(d, f), \"rb\") as fin, open(os.path.join(d, f.replace(\".out\", \".title\")), \"rb\") as tfin:\n",
    "        dct = pickle.load(tfin)\n",
    "        while True:\n",
    "            pbar.update(1)\n",
    "            try:\n",
    "                document = pickle.load(fin)\n",
    "            except:\n",
    "                break\n",
    "            processor.process(document, dct[document.id])\n",
    "    processor.process_pack()\n",
    "\n",
    "def process_collection(directory, processor):\n",
    "    pbar = tqdm(total = 200000)\n",
    "    for file in os.listdir(directory):\n",
    "        if (file.endswith(\".out\")):\n",
    "            process_file(directory, file, processor, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "def stem_queries(queries):\n",
    "    m = Mystem(grammar_info=False, disambiguation=False)\n",
    "    reg = re.compile('[a-zа-яё0-9\\-]')\n",
    "    filterFunc = lambda w : reg.match(w)\n",
    "    result = {}\n",
    "    for (qid, text) in queries.items():\n",
    "        result[qid] = ' '.join(filter(filterFunc, m.lemmatize(text)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi #https://pypi.org/project/rank-bm25/ examples\n",
    "from rank_bm25 import BM25Plus\n",
    "\n",
    "def __feature_bm25(documents, queries, BM, result):\n",
    "    all_words = [doc for doc in documents]\n",
    "    bm25 = BM(all_words)\n",
    "    for qid, q in queries.items():\n",
    "        print(qid)\n",
    "        if qid not in result:\n",
    "            result[qid] = []\n",
    "        result[qid].extend(bm25.get_scores(q.split(\" \")))\n",
    "    return result\n",
    "\n",
    "def feature_bm25Plus(documents, queries, result):\n",
    "    return __feature_bm25(documents, queries, BM25Plus, result)\n",
    "\n",
    "def feature_bm25(documents, queries, result):\n",
    "    return __feature_bm25(documents, queries, BM25Okapi, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QWord:\n",
    "    def __init__(self, word, pos, prev = None):\n",
    "        self.prev = prev\n",
    "        self.next = None\n",
    "        self.word = word\n",
    "        self.pos = pos\n",
    "        self.step = 0\n",
    "        if (prev != None):\n",
    "            prev.next = self\n",
    "            self.step = pos - prev.pos\n",
    "\n",
    "def check(d):\n",
    "    for k, v in d.items():\n",
    "        if v == 0:\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "def feature_window(document, query, title):\n",
    "    queryset = set(query.split(' '))\n",
    "    head = None\n",
    "    tail = None\n",
    "    for i, w in enumerate(document.words):\n",
    "        if w in queryset:\n",
    "            if head == None:\n",
    "                head = QWord(w, i)\n",
    "                tail = head\n",
    "            else:\n",
    "                tail = QWord(w, i, tail)\n",
    "                \n",
    "    if not head:\n",
    "        return document.sz_bytes\n",
    "                \n",
    "    collect = dict.fromkeys(queryset, 0)\n",
    "    node1 = head\n",
    "    node2 = head\n",
    "    collect[head.word] = 1\n",
    "    length = 0\n",
    "    minLength = len(document.words)\n",
    "    \n",
    "    while node1.next != None:\n",
    "        node1 = node1.next\n",
    "        collect[node1.word] += 1\n",
    "        length += node1.step\n",
    "        while node2.next != None and check(collect):\n",
    "            minLength = min(minLength, length)\n",
    "            collect[node2.word] -= 1\n",
    "            node2 = node2.next\n",
    "            length -= node2.step\n",
    "    return minLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_query_len(document, query, title):\n",
    "    return len(query)\n",
    "\n",
    "def feature_query_list_len(document, query, title):\n",
    "    return len(query.split(' '))\n",
    "\n",
    "def feature_doc_len(document, query, title):\n",
    "    return len(document.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_urls = {}\n",
    "\n",
    "class GetDocUrls(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "    \n",
    "    def process(self, document, title):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        document_url = str(document.url)[2:-1].split(\"?\")[0].split(\"#\")[0]\n",
    "        if document_url[-1] == '/':\n",
    "            document_url = document_url[:-1]\n",
    "        document_urls[document_url] = True\n",
    "        \n",
    "    def result(self):\n",
    "        pass\n",
    "        \n",
    "processor = GetDocUrls()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "pagerank_dict = {}\n",
    "\n",
    "class GraphBuider(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "        self.edge_list = []\n",
    "    \n",
    "    def process(self, document, title):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        links = document.links\n",
    "        document_url = str(document.url)[2:-1].split(\"?\")[0].split(\"#\")[0]\n",
    "        if document_url[-1] == '/':\n",
    "            document_url = document_url[:-1]\n",
    "        links_absolute_no_params = []\n",
    "        for link in links:\n",
    "            if not link:\n",
    "                continue\n",
    "            link_absolute_path = link    \n",
    "            if not (link_absolute_path.startswith(\"http://\") or link_absolute_path.startswith(\"https://\")):\n",
    "                if link_absolute_path[0] == '.':\n",
    "                    link_absolute_path = link_absolute_path[2:]\n",
    "                link_absolute_path = document_url.rsplit(\"/\", 1)[0] + \"/\" + link_absolute_path\n",
    "            link_absolute_no_params = link_absolute_path.split(\"?\")[0].split(\"#\")[0]\n",
    "            if link_absolute_no_params[-1] == '/':\n",
    "                    link_absolute_no_params = link_absolute_no_params[:-1]\n",
    "            if not (link_absolute_no_params in links_absolute_no_params):\n",
    "                links_absolute_no_params.append(link_absolute_no_params)\n",
    "        for link in links_absolute_no_params:\n",
    "            if link in document_urls:\n",
    "                self.edge_list.append((document_url, link))\n",
    "        \n",
    "    def result(self):\n",
    "        graph = nx.DiGraph(self.edge_list)\n",
    "        return nx.pagerank(graph)\n",
    "        \n",
    "processor = GraphBuider()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "pagerank_dict = processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_id = {}\n",
    "\n",
    "class GetDocUrlsToIds(BaseDocumentProcessor):\n",
    "    def __init__(self):\n",
    "        \"\"\" do all initialization here \"\"\"\n",
    "    \n",
    "    def process(self, document, title):\n",
    "        \"\"\" document: Document (see first cell)\n",
    "            process each document here \"\"\"\n",
    "        url_to_id[document.url.decode(\"cp1251\")] = document.id\n",
    "        \n",
    "    def result(self):\n",
    "        pass\n",
    "        \n",
    "processor = GetDocUrlsToIds()        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "processor.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_relevance(file, r_type):\n",
    "    did = []\n",
    "    qid = []\n",
    "    y = []\n",
    "    with open(file, \"rt\", encoding=\"cp1251\") as file:\n",
    "        bs = BeautifulSoup(file.read())\n",
    "        for task in bs.find_all(\"task\"):\n",
    "            for doc in task.find_all(\"document\"):\n",
    "                if (r_type == 2008):\n",
    "                    if (doc[\"id\"] not in url_to_id):\n",
    "                        continue\n",
    "                    did.append(url_to_id[doc[\"id\"]])\n",
    "                else:\n",
    "                    did.append(int(doc[\"id\"]))\n",
    "                qid.append(int(task[\"id\"][3:]))\n",
    "                y.append(1 if doc[\"relevance\"] == \"vital\" else 0)\n",
    "    return did, qid, y\n",
    "\n",
    "def read_queries(file):\n",
    "    queries = {}\n",
    "    with open(file, \"rt\", encoding=\"cp1251\") as file:\n",
    "        bs = BeautifulSoup(file.read())\n",
    "        for task in bs.find_all(\"task\"):\n",
    "            queries[int(task[\"id\"][3:])] = task.text\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did, qid, y = read_relevance(\"../relevant_table_2008.xml\", 2008)\n",
    "queries = read_queries(QUERIES_FILE)\n",
    "queries = stem_queries(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetAllMetricsProcessor(BaseDocumentProcessor):\n",
    "    def __init__(self, queries):\n",
    "        self.queries = queries\n",
    "        self.docs = {}\n",
    "        self.bm25_plus = {}\n",
    "        self.doc_len = []\n",
    "        self.window = {}\n",
    "        self.id_to_ind = {}\n",
    "    \n",
    "    def process(self, document, title):\n",
    "        self.docs[document.id] = document.words\n",
    "        self.doc_len.append(feature_doc_len(document, None, title))\n",
    "        self.id_to_ind[document.id] = len(self.id_to_ind)\n",
    "        for query in self.queries.keys():\n",
    "            if query not in self.window:\n",
    "                self.window[query] = []\n",
    "            self.window[query].append(feature_window(document, queries[query], title))\n",
    "\n",
    "    def process_pack(self):\n",
    "        feature_bm25Plus(self.docs.values(), self.queries, self.bm25_plus)\n",
    "        self.docs = {}\n",
    "        \n",
    "    def result(self):\n",
    "        return self.bm25_plus, self.doc_len, self.window, self.id_to_ind\n",
    "\n",
    "processor = GetAllMetricsProcessor(queries)        \n",
    "process_collection(COLLECTION_DIRECTORY, processor)\n",
    "m1, m2, m3, id_to_ind = processor.result()\n",
    "m4 = pagerank_dict\n",
    "m5 = {}\n",
    "m6 = {}\n",
    "\n",
    "for q in queries.keys():\n",
    "    m5[q] = feature_query_len(None, queries[q], None)\n",
    "    m6[q] = feature_query_list_len(None, queries[q], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "id_to_url = {}\n",
    "for uurl, iid in url_to_id.items():\n",
    "    id_to_url[iid] = uurl\n",
    "\n",
    "x = []\n",
    "for i in range(len(y)):\n",
    "    v = np.zeros((6,))\n",
    "    v[0] = m1[qid[i]][id_to_ind[did[i]]]\n",
    "    v[1] = m2[id_to_ind[did[i]]]\n",
    "    v[2] = m3[qid[i]][id_to_ind[did[i]]]\n",
    "    v[3] = m4[id_to_ind[id_to_url[did[i]]]]\n",
    "    v[4] = m5[qid[i]]\n",
    "    v[5] = m6[qid[i]]\n",
    "    x.append(v)\n",
    "\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, qid, train_ratio):\n",
    "    n = qid.shape[0]\n",
    "    train_inds = np.sort(np.random.choice(n, int(n * train_ratio), replace=False))\n",
    "    test_inds = np.sort(np.setdiff1d(range(n), train_inds))\n",
    "    return (\n",
    "        x[train_inds], y[train_inds], qid[train_inds],\n",
    "        x[test_inds], y[test_inds], qid[test_inds],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = np.array(qid)\n",
    "y = np.array(y)\n",
    "train_x, train_y, train_qid, test_x, test_y, test_qid = split(x, y, qid, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "train = Pool(data=train_x, label=train_y, group_id=train_qid)\n",
    "test = Pool(data=test_x, label=test_y, group_id=test_qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoost\n",
    "from statistics import mean\n",
    "\n",
    "parameters = { 'custom_metric': ['NDCG:top=20'], 'iterations': 2000, 'loss_function': 'PairLogitPairwise' }\n",
    "\n",
    "model = CatBoost(parameters)\n",
    "model.fit(train, eval_set=test)\n",
    "print(\"train: \", mean(model.eval_metrics(train, ['NDCG:top=20'])['NDCG:top=20;type=Base']))\n",
    "print(\"test: \", mean(model.eval_metrics(test, ['NDCG:top=20'])['NDCG:top=20;type=Base']))\n",
    "print(model.get_feature_importance())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
